{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms \n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Re = 400\n",
    "bs = 16\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetUV(Dataset):\n",
    "    def __init__(self, root_dir, data_transform=None, label_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.data_transform = data_transform\n",
    "        self.label_transform = label_transform\n",
    "        self.image_paths = []\n",
    "        self.labels_path = []\n",
    "        image_dir = os.path.join(root_dir, 'Mesh')\n",
    "        u_labels_dir = os.path.join(root_dir, 'W_Data')\n",
    "        v_labels_dir = os.path.join(root_dir, 'W_Data')\n",
    "        for file in os.listdir(image_dir):\n",
    "            Ulabel_file = 'U' + file[4:]\n",
    "            Vlabel_file = 'U' + file[4:]\n",
    "            if file.endswith('.png'):\n",
    "                self.image_paths.append(os.path.join(image_dir, file))\n",
    "                self.labels_path.append((os.path.join(u_labels_dir, Ulabel_file), os.path.join(v_labels_dir, Vlabel_file)))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label_path = self.labels_path[idx]\n",
    "        image = Image.open(img_path).convert('L')\n",
    "        Ulabel = Image.open(label_path[0])\n",
    "        Ulabel = Ulabel.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        Ulabel = Ulabel.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        Vlabel = Image.open(label_path[1])\n",
    "        Vlabel = Vlabel.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        Vlabel = Vlabel.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "        image = self.data_transform(image)\n",
    "        Ulabel = self.label_transform(Ulabel)\n",
    "        Vlabel = self.label_transform(Vlabel)\n",
    "        label = torch.concat((Ulabel, Vlabel), dim=0)\n",
    "        return image, label\n",
    "    \n",
    "class CustomDatasetMag(Dataset):\n",
    "    def __init__(self, root_dir, data_transform=None, label_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.data_transform = data_transform\n",
    "        self.label_transform = label_transform\n",
    "        self.image_paths = []\n",
    "        self.labels_path = []\n",
    "        image_dir = os.path.join(root_dir, 'Mesh')\n",
    "        labels_dir = os.path.join(root_dir, 'W_Data')\n",
    "        for file in os.listdir(image_dir):\n",
    "            label_file = 'U' + file[4:]\n",
    "            if file.endswith('.png'):\n",
    "                self.image_paths.append(os.path.join(image_dir, file))\n",
    "                self.labels_path.append(os.path.join(labels_dir, label_file))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label_path = self.labels_path[idx]\n",
    "        image = Image.open(img_path).convert('L')\n",
    "        label = Image.open(label_path)\n",
    "\n",
    "        image = self.data_transform(image)\n",
    "        label = self.label_transform(label)\n",
    "        return image, label\n",
    "    \n",
    "    # Define transformations\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "label_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "# Path to your dataset folder\n",
    "dataset_root = \".\\\\\"\n",
    "\n",
    "# Create dataset\n",
    "dataset = CustomDatasetMag(dataset_root, data_transform=img_transform, label_transform = label_transform)\n",
    "\n",
    "# Split dataset into train and validation\n",
    "dataset_size = len(dataset)\n",
    "val_split = 0.2\n",
    "val_size = int(val_split * dataset_size)\n",
    "train_size = dataset_size - val_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max']):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.gate_channels = gate_channels\n",
    "        self.pool_types = pool_types\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(gate_channels, gate_channels // reduction_ratio),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(gate_channels // reduction_ratio, gate_channels)\n",
    "            )\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        channel_att_sum = 0\n",
    "        for pool_type in self.pool_types:\n",
    "            if pool_type=='avg':\n",
    "                avg_pool = self.avg_pool(x)\n",
    "                channel_att_raw = self.mlp(avg_pool)\n",
    "            elif pool_type=='max':\n",
    "                max_pool = self.max_pool(x)\n",
    "                channel_att_raw = self.mlp(max_pool)\n",
    "\n",
    "        channel_att_sum = channel_att_sum + channel_att_raw\n",
    "\n",
    "        scale = self.sigmoid(x).expand_as(x)\n",
    "        return x * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input):\n",
    "        avg_out = torch.mean(input, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(input, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return input * self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Down(nn.Module) :\n",
    "    '''\n",
    "    Encoder Block\n",
    "    '''\n",
    "    def __init__(self, in_ch, out_ch) : \n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=4, padding=1, stride= 2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(out_ch)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Up(nn.Module):\n",
    "    def __init__(self, out_size, insize, upsize, att_type='channel'):  #channel or spatial\n",
    "        super(Up, self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.ConvTranspose2d(insize, upsize, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(upsize),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        if att_type == 'channel':\n",
    "            self.att = ChannelAttention(256)\n",
    "        elif att_type == 'spatial':\n",
    "            self.att = SpatialAttention()\n",
    "        else:\n",
    "            self.att = nn.Identity()\n",
    "\n",
    "    def forward(self, inputs1, inputs2):\n",
    "        inputs1_att = self.att(inputs1)\n",
    "        outputs = torch.cat([inputs1_att, self.up(inputs2)], 1)\n",
    "        # outputs = self.conv1(outputs)\n",
    "        # outputs = self.att(outputs) * outputs\n",
    "        # outputs = self.conv2(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetAM(nn.Module):\n",
    "    def __init__(self, in_channels=1):\n",
    "        super(UnetAM, self).__init__()                                   # 64 512\n",
    "\n",
    "        self.down1 = Down(in_channels, 16)\n",
    "        self.down2 = Down(16, 32)\n",
    "        self.down3 = Down(32, 32)\n",
    "        self.down4 = Down(32, 64)\n",
    "        self.down5 = Down(64, 64)\n",
    "        self.down6 = Down(64, 256)\n",
    "        self.down7 = Down(256, 256)\n",
    "\n",
    "        self.c_att = ChannelAttention(256)\n",
    "\n",
    "        self.up = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        # upsampling  #channel or spatial\n",
    "        self.up_concat6 = Up(512, 256, 256, att_type='spatial')\n",
    "        self.up_concat5 = Up(128, 512, 64, att_type='spatial')\n",
    "        self.up_concat4 = Up(128, 128, 64, att_type='spatial')\n",
    "        self.up_concat3 = Up(64, 128, 32, att_type='spatial')\n",
    "        self.up_concat2 = Up(64, 64, 32, att_type='spatial')\n",
    "        self.up_concat1 = Up(32, 64, 16, att_type='spatial')\n",
    "        self.final = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, in_channels * 3, 4, 2, 1, bias=False),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        h, w = inputs.shape[2], inputs.shape[3]\n",
    "        feat1 = self.down1(inputs)  # (128, 256, 256)\n",
    "        feat2 = self.down2(feat1)  # (128, 128, 128)\n",
    "        feat3 = self.down3(feat2)  # (256, 64, 64)\n",
    "        feat4 = self.down4(feat3)  # (256, 32, 32)\n",
    "        feat5 = self.down5(feat4)  # (512, 16, 16)\n",
    "        feat6 = self.down6(feat5)\n",
    "        feat7 = self.down7(feat6)\n",
    "        feat7 = self.c_att(feat7) + feat7\n",
    "\n",
    "        up6 = self.up_concat6(feat6, feat7)\n",
    "        up5 = self.up_concat5(feat5, up6)  # (256, 32, 32)\n",
    "        up4 = self.up_concat4(feat4, up5)\n",
    "        up3 = self.up_concat3(feat3, up4)\n",
    "        up2 = self.up_concat2(feat2, up3)\n",
    "        up1 = self.up_concat1(feat1, up2)\n",
    "        final = self.final(up1)\n",
    "\n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DataLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, true, pred):\n",
    "        data_loss = (torch.abs(true - pred).sum()) / (6 * true.shape[0] * true.shape[2] * true.shape[3])\n",
    "        return data_loss\n",
    "    \n",
    "class PhysicsLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PhysicsLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, true, pred):\n",
    "        ui = true[:, 0, 1:-1, 1:-1]\n",
    "        vi = true[:, 1, 1:-1, 1:-1]\n",
    "        ut = pred[:, 0, 1:-1, 1:-1]\n",
    "        vt = pred[:, 1, 1:-1, 1:-1]\n",
    "\n",
    "        dui_dx = (true[:, 0, 2:, 1:-1] - true[:, 0, :-2, 1:-1] ) / 2\n",
    "        dui_dy = (true[:, 0, 1:-1:, 2:] - true[:, 0, 1:-1, :-2] ) / 2\n",
    "        dvi_dy = (true[:, 1, 1:-1, 2:] - true[:, 1, 1:-1, :-2] ) / 2\n",
    "        dvi_dx = (true[:, 1, 2:, 1:-1] - true[:, 1, :-2, 1:-1] ) / 2\n",
    "        dut_dx = (pred[:, 0, 2:, 1:-1] - pred[:, 0, :-2, 1:-1] ) / 2\n",
    "        dut_dy = (pred[:, 0, 1:-1:, 2:] - pred[:, 0, 1:-1, :-2] ) / 2\n",
    "        dvt_dy = (pred[:, 1, 1:-1, 2:] - pred[:, 1, 1:-1, :-2] ) / 2\n",
    "        dvt_dx = (pred[:, 1, 2:, 1:-1] - pred[:, 1, :-2, 1:-1] ) / 2\n",
    "\n",
    "        d2ui_dx2 = (true[:, 0, 2:, 1:-1] - 2 * true[:, 0, 1:-1, 1:-1]  + true[:, 0, :-2, 1:-1]) \n",
    "        d2ui_dy2 = (true[:, 0, 1:-1, 2:] - 2 * true[:, 0, 1:-1, 1:-1]  + true[:, 0, 1:-1, :-2]) \n",
    "        d2vi_dy2 = (true[:, 1, 1:-1, 2:] - 2 * true[:, 1, 1:-1, 1:-1] + true[:, 1, 1:-1, :-2])\n",
    "        d2vi_dx2 = (true[:, 1, 2:, 1:-1] - 2 * true[:, 1, 1:-1, 1:-1]  + true[:, 1, :-2, 1:-1])\n",
    "        d2ut_dx2 = (pred[:, 0, 2:, 1:-1] - 2 * pred[:, 0, 1:-1, 1:-1]  + pred[:, 0, :-2, 1:-1]) \n",
    "        d2ut_dy2 = (pred[:, 0, 1:-1, 2:] - 2 * pred[:, 0, 1:-1, 1:-1]  + pred[:, 0, 1:-1, :-2]) \n",
    "        d2vt_dy2 = (pred[:, 1, 1:-1, 2:] - 2 * pred[:, 1, 1:-1, 1:-1] + pred[:, 1, 1:-1, :-2])\n",
    "        d2vt_dx2 = (pred[:, 1, 2:, 1:-1] - 2 * pred[:, 1, 1:-1, 1:-1]  + pred[:, 1, :-2, 1:-1])\n",
    "\n",
    "        mass_loss = torch.abs((dui_dx + dvi_dy) - (dut_dx + dvt_dy)).sum() / (true.shape[0] * (true.shape[2] - 2) * (true.shape[3] - 2))\n",
    "\n",
    "        momentum_loss_x = torch.abs(((2 * dui_dx + ui * dvi_dy + vi * dui_dy) - (d2ui_dx2 + d2ui_dy2) / Re) - ((2 * dut_dx + ut * dvt_dy + vt * dut_dy) - (d2ut_dx2 + d2ut_dy2) / Re))\n",
    "        momentum_loss_y = torch.abs(((2 * dvi_dy + ui * dvi_dx + vi * dui_dx) - (d2vi_dx2 + d2vi_dy2) / Re) - ((2 * dvt_dy + ut * dvt_dx + vt * dut_dx) - (d2vt_dx2 + d2vt_dy2) / Re))\n",
    "\n",
    "        total_momentum_loss = (momentum_loss_x + momentum_loss_y).sum() / (true.shape[0] * (true.shape[2] - 2) * (true.shape[3] - 2))\n",
    "        return mass_loss, total_momentum_loss\n",
    "    \n",
    "class Loss(nn.Module):\n",
    "    def __init__(self, alpha_1 = 0.33, alpha_2 = 1.67, alpha_3 = 8.33) -> None:\n",
    "        super(Loss, self).__init__()\n",
    "        self.a1 = alpha_1\n",
    "        self.a2 = alpha_2\n",
    "        self.a3 = alpha_3\n",
    "        self.data_loss = DataLoss()\n",
    "        self.physics_loss = PhysicsLoss()\n",
    "\n",
    "    def forward(self, true, pred):\n",
    "        loss_data = self.data_loss(true, pred)\n",
    "        loss_mass, loss_momentum = self.physics_loss(true, pred)\n",
    "\n",
    "        total_loss = self.a1 * loss_data + self.a2 * loss_mass + self.a3 * loss_momentum\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MRE(true, pred):\n",
    "    return ((1 / true.shape[0]) * ((torch.abs(true - pred)).sum()) / true.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetAM_Module(L.LightningModule):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.model = UnetAM()\n",
    "        self.loss = DataLoss()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=1e-4)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.1)\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        preds = self.model(imgs)\n",
    "        loss = self.loss(labels, preds)\n",
    "        mre = MRE(labels, preds)\n",
    "\n",
    "        self.log(\"train/loss\", loss)\n",
    "        self.log(\"train/mre\", mre)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        preds = self.model(imgs)\n",
    "        loss = self.loss(labels, preds)\n",
    "        mre = MRE(labels, preds)\n",
    "\n",
    "        self.log(\"val/loss\", loss)\n",
    "        self.log(\"val/mre\", mre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type     | Params\n",
      "-----------------------------------\n",
      "0 | model | UnetAM   | 4.3 M \n",
      "1 | loss  | DataLoss | 0     \n",
      "-----------------------------------\n",
      "4.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.3 M     Total params\n",
      "17.259    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 181/181 [01:26<00:00,  2.08it/s, v_num=14]      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 181/181 [01:26<00:00,  2.08it/s, v_num=14]\n"
     ]
    }
   ],
   "source": [
    "model = UnetAM_Module()\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val/mre\", mode=\"min\")\n",
    "trainer = L.Trainer(max_epochs=epochs,  callbacks=[checkpoint_callback])\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
